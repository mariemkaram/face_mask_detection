# -*- coding: utf-8 -*-
"""Face mask detection .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkrWJfeQ_oJbV98oKqPwFPprrszeDDqE
"""

pip install scikit-image

pip install opencv-python

import cv2
import numpy as np
import skimage.io as io
import matplotlib.pyplot as plt
from skimage.io import imread
from skimage.transform import resize
from skimage.feature import hog
# from skimage.color import rgb2grey
from skimage import exposure
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn import svm, datasets

import tensorflow as tf
from tensorflow import keras

from google.colab import drive
drive.mount('/content/drive')

path='drive/MyDrive/data'

categories=['with_mask','without_mask']

# data=os.path.join(path,categories[0])
# data
# os.listdir(data)
# dataimage=os.path.join(data,os.listdir(data)[0])
# dataimage
# img=imread(imagePath)
# resized_img=resize(img,(128,64))
# if len(img.shape)>2:
#             fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),
#                 	cells_per_block=(2, 2), visualize=True,multichannel=True)
# hog_image

featurelist=[]
target_arr=[]
# data=os.path.join(path,category[0])
for category in categories:
    data=os.path.join(path,category)
    for i in os.listdir(data):
        dataimage=os.path.join(data,i)
        if i=='.ipynb_checkpoints':
            continue
        imagePath=os.path.join(data,i)
#         print(imagePath)
        img=imread(imagePath)
        resized_img=resize(img,(128,64))
        if len(img.shape)>2:
            fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),
                	cells_per_block=(2, 2), visualize=True,multichannel=True)
#             print(len(fd.shape))
        else:
            fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),
                	cells_per_block=(2, 2), visualize=True)
#             print(len(fd.shape))
        featurelist.append(fd)
#         print(cat)
        target_arr.append(category)
matrix=np.array(featurelist)
matrix1=np.array(target_arr)
df=pd.DataFrame(matrix)

df['target']= matrix1

print(df.shape)

df.info()

X=df.iloc[:,:-1]
Y=df.iloc[:,-1]
Y = Y.replace('without_mask', 0)
Y = Y.replace('with_mask', 1)

X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.2, random_state=42)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)
Y

# Creating a model

# Getting the input shape

input_shape = X_train.shape[1:]

model_1 = tf.keras.models.Sequential([

              # The first layer has 30 neurons(or units)
              tf.keras.layers.Dense(units=30, input_shape=input_shape, activation='relu'),

              # The second layer has 25 neurons

              tf.keras.layers.Dense(units=15, activation='relu'),

              # The third layer has 1 neuron and activation of sigmoid.
              # Because of sigmoid, the output of this layer will be a value bwteen 0 and 1
              tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compiling the model

model_1.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics='accuracy')

# By setting validation_split=0.15, I am allocating 15% of the dataset to be used for evaluating the model during the training
# Model training returns model history(accuracy, loss, epochs...)

history = model_1.fit(X_train, Y_train, epochs=60, validation_split=0.15)

# Getting the dataframe of loss and accuracies on both training and validation

loss_acc_metrics_df = pd.DataFrame(history.history)
loss_acc_metrics_df.plot(figsize=(10,5))

# Evaluating a model on unseen data: test set

model_eval = model_1.evaluate(X_test, Y_test)

# Printing the loss and accuracy

print('Test loss: {}\nTest accuracy:{}'.format(model_eval[0],model_eval[1]))

# â€¼DON'T DO THIS!! X_test is not scaled. The results will be awful

model_1.evaluate(X_test, Y_test)

# Getting the prediction

predictions = model_1.predict(X_test)
# Rounding the predictions to 0 and 1

predictions = tf.round(predictions)
predictions

# Getting the confusion matrix
import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, predictions)

# Plotting confusion matrix

plt.figure(figsize=(6,6))
sns.heatmap(cm, square=True, annot=True, fmt='d', cbar=True,
                        xticklabels=['0: without_mask', '1:with_mask'],
                        yticklabels=['0: without_mask', '1:with_mask'])
plt.ylabel('Actual label')
plt.xlabel('Predicted label');